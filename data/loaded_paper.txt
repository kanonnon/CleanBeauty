LISA: Reasoning Segmentation via Large Language ModelXin Lai1*Zhuotao Tian2∗†Yukang Chen1Yanwei Li1Yuhui Yuan4Shu Liu3Jiaya Jia1,31CUHK2HIT (Shenzhen)3SmartMore4MSRAAbstractAlthough perception systems have made remarkable ad-vancements in recent years, they still rely on explicit humaninstruction or pre-defined categories to identify the targetobjects before executing visual recognition tasks. Such sys-tems cannot actively reason and comprehend implicit userintention. In this work, we propose a new segmentationtask — reasoning segmentation. The task is designed tooutput a segmentation mask given a complex and implicitquery text. Furthermore, we establish a benchmark com-prising over one thousand image-instruction-mask data sam-ples, incorporating intricate reasoning and world knowledgefor evaluation purposes. Finally, we present LISA: largeLanguage Instructed Segmentation Assistant, which inheritsthe language generation capabilities of multimodal Large
Language Models (LLMs) while also possessing the abilityto produce segmentation masks. We expand the originalvocabulary with a <SEG> token and propose the embedding-as-mask paradigm to unlock the segmentation capability.Remarkably, LISA can handle cases involving complex rea-soning and world knowledge. Also, it demonstrates robustzero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely239 reasoning segmentation data samples results in furtherperformance enhancement. Both quantitative and qualita-tive experiments show our method effectively unlocks newreasoning segmentation capabilities for multimodal LLMs.Code, models, and data are available at github.com/dvlab-research/LISA.1. IntroductionIn daily life, users tend to issue direct commands like“Change the TV channel” to instruct a robot, rather than pro-viding explicit step-by-step instructions such as “Go to the
table first, find the TV remote, and then press the button tochange the channel.” However, existing perception systemsconsistently rely on humans to explicitly indicate target ob-jects or pre-define categories before executing visual recog-*Equal Contribution†Corresponding Author (tianzhuotao@hit.edu.cn).nition tasks. These systems cannot actively reason and com-prehend user intention based on implicit instruction. Thisreasoning ability is crucial in developing next-generationintelligent perception systems and holds substantial potentialfor industrial applications, particularly in robotics.In this work, we introduce a new segmentation task —reasoning segmentation , which requires generating a binarysegmentation mask based on an implicit query text involvingcomplex reasoning . Notably, the query text is not limitedto a straightforward reference (e.g., “the orange”), but amore complicated description involving complex reasoning
orworld knowledge (e.g., “the food with high Vitamin C”).To accomplish this task, the model must possess two keyabilities: 1) reasoning complex andimplicit text queriesjointly with the image; 2) producing segmentation masks.Inspired by the exceptional capacity of LLMs to reasonand comprehend user intentions, we aim to leverage thiscapability of LLMs to address the aforementioned first chal-lenge. However, while several studies [ 1,23,24,28,29,55,63] have integrated robust reasoning capabilities into multi-modal LLMs to accommodate visual input, the majority ofthese models primarily concentrate on text generation tasksand still fall short in performing vision tasks that requirefine-grained output formats, such as segmentation masks.This leads us to ask: can we enable multimodal LLMs withthe capability to output segmentation masks?To this end, we introduce LISA: a large LanguageInstructed Segmentation Assistant, a multimodal LLM ca-
pable of producing segmentation masks. Specifically, weincorporate an additional token, i.e., <SEG> , into the ex-
Instructed Segmentation Assistant, a multimodal LLM ca-pable of producing segmentation masks. Specifically, weincorporate an additional token, i.e., <SEG> , into the ex-isting vocabulary. Upon generating the <SEG> token, itshidden embedding is further decoded into the correspondingsegmentation mask. By representing the segmentation maskas an embedding, LISA acquires segmentation capabilitiesand benefits from end-to-end training. Remarkably, LISAdemonstrates robust zero-shot abilities. Training the modelsolely on standard semantic segmentation and referring seg-mentation datasets yields surprisingly effective performanceon the reasoning segmentation task. Furthermore, we findthat LISA’s performance can be significantly enhanced byfine-tuning on just 239 reasoning segmentation data samples.As illustrated in Fig. 1, LISA can handle various scenariosarXiv:2308.00692v3  [cs.CV]  1 May 2024
•USER: <IMAGE> Can you segmentthe unusual partin this image and explain why?•ASSISTANT: Sure, it is <SEG>. In the image, the unusual part isthe dog wearing a reindeer antler headband.•USER: <IMAGE> There are two washing machines as shown in the picture. If I need to do laundry, where in the picture would I put the clothes? Please output segmentation mask.•ASSISTANT: <SEG>.•USER: <IMAGE> Can you segment the food that tastes not spicyin this image? •ASSISTANT: <SEG>.•USER: <IMAGE> What is the place where the driver can observe the speedin this image? Please output segmentation mask. •ASSISTANT: <SEG>.•USER: <IMAGE> Who is the loserin this image? Please output segmentation mask and explain why. •ASSISTANT: Sure, it is <SEG>. ... The man on top is likely the winner of the match, as he has control over the other fighters body...•USER: <IMAGE> Can you segment the tyrethat does not touch the groundin this image? •ASSISTANT: Sure, it is <SEG>.
•USER: <IMAGE> Can you segment the fried foodand the food with most proteinseparately in this image?•ASSISTANT: Sure, they are <SEG> and <SEG>.Figure 1. We unlock new segmentation capabilities for existing multimodal LLMs. Our model (i.e., LISA) can deal with cases involvingcomplex reasoning and world knowledge. Also, we demonstrate the cases of explanatory answers in the 3rd row. Additionally, in the 4throw, our model can output multiple segmentation masks in a single answer. More illustrations can be found in the supplementary material.involving complex reasoning and world knowledge.In addition, to validate the effectiveness, we estab-lish a benchmark for reasoning segmentation evaluation,called ReasonSeg . Comprising over one thousand image-instruction pairs, this benchmark offers persuasive evalua-tion metrics for the task. To align more closely with practicalapplications, we annotate the images from OpenImages [ 21]
and ScanNetv2 [ 10] with implicit text queries that involvecomplex reasoning.In summary, our contributions are as follows:•We introduce the reasoning segmentation task, whichnecessitates reasoning based on implicit human instruc-tions. Such reasoning capability is crucial for buildinga genuinely intelligent perception system.•We present our model — LISA, which incorporates newsegmentation capabilities. It demonstrates robust zero-shot ability on the reasoning segmentation task whentrained solely on reasoning-free datasets, and achievesfurther performance boost by fine-tuning on just 239data samples that involve reasoning.•We establish a reasoning segmentation benchmark,ReasonSeg , containing over one thousand image-instruction-mask data samples. This benchmark is es-sential for evaluation and encourages the community tofurther explore the reasoning ability for vision tasks.2. Related Work2.1. Image Segmentation
2.1. Image SegmentationSemantic segmentation aims to assign a class label to ev-ery pixel in an image. Numerous studies [ 2,5,8,12,16,22,31,37,42,43,45,46,51,56,59–61,64] have pro-posed diverse designs (such as encoder-decoder, dilatedconvolution, pyramid pooling module, non-local operator,and more) to effectively encode semantic information. Re-search on instance segmentation [ 9,14,58] and panopticsegmentation [ 7,18,25,50] has introduced various architec-tural innovations for instance-level segmentation, includingDETR [ 4]-based structures, mask attention, and dynamicconvolution. In recent years, typical segmentation tasks havemade significant progress and become increasingly mature.Consequently, it is imperative to develop more intelligentinteraction ways for image segmentation.The referring segmentation task [ 17,36] enables inter-action with human language, aiming to segment the targetobject based on a given explicit text description. Recently,
Kirillov et al. [19] introduced SAM, trained with billions ofhigh-quality masks, supporting bounding boxes and pointsas prompts while demonstrating exceptional segmentation
Kirillov et al. [19] introduced SAM, trained with billions ofhigh-quality masks, supporting bounding boxes and pointsas prompts while demonstrating exceptional segmentationquality. X-Decoder [ 65] bridges vision and language, unify-ing multiple tasks within a single model. SEEM [ 66] furthersupports various human interaction methods, including text,audio, and scribble. However, these studies primarily focus
the camera lensthat is more suitable for photographing nearby objectsToddlers are curious and often enjoy exploring their surroundings. What object in the picture can provide a safe and enclosed space for a toddler to play in? Figure 2. Examples of the annotated image-instruction-mask data samples. Left: short phrase query. Right: long sentence query. Moreexamples are given in the supplementary material.on addressing multi-task compatibility and unification, ne-glecting the injection of new capabilities. In this work, wepresent LISA and it possesses reasoning ability that has notbeen explored yet in existing segmentors.2.2. Multimodal Large Language ModelMotivated by the remarkable reasoning abilities of LLMs,researchers are exploring ways to transfer these capabili-ties into the vision domain, developing multimodal LLMs.Flamingo [ 1] employs a cross-attention structure to attend tovisual contexts, enabling visual in-context learning. Models
such as BLIP-2 [ 24] and mPLUG-OWL [ 55] propose en-coding image features with a visual encoder, which are thenfed into the LLM alongside text embeddings. Otter [ 23]further incorporates robust few-shot capabilities throughin-context instruction tuning on the proposed MIMIC-ITdataset. LLaV A [ 29] and MiniGPT-4 [ 63] first conductimage-text feature alignment followed by instruction tun-ing. Koh et al. [20] also investigates image retrieval forLLMs. Moreover, numerous works [ 32,44,49,52,54] uti-lize prompt engineering, connecting independent modulesvia API calls, but without the benefits of end-to-end training.Recently, there have been studies examining the intersec-tion between multimodal LLMs and vision tasks. Vision-LLM [ 47] offers a flexible interaction interface for multiplevision-centric tasks through instruction tuning but fails tofully exploit LLMs for complex reasoning. Kosmos-2 [ 38]constructs large-scale data of grounded image-text pairs,
infusing grounding capabilities into LLMs. DetGPT [ 39]bridges the fixed multimodal LLM and open-vocabulary de-tector, enabling detection to be performed based on userinstruction. GPT4RoI [ 57] introduces spatial boxes as inputand trains the model on region-text pairs. In contrast, ourwork aims to efficiently inject segmentation capabilities intomultimodal LLMs in the manner of end-to-end training.3. Reasoning Segmentation3.1. Problem DefinitionThe reasoning segmentation task is to output a binary seg-mentation mask M, given an input image ximgand an im-plicit query text instruction xtxt. The task shares a similarformulation with the referring segmentation task [ 17], but isfar more challenging. The key distinction lies in the complex-ity of the query text in reasoning segmentation. Instead of astraightforward phrase (e.g., “the trash can”), the query textincludes more intricate expressions (e.g., “something that the
garbage should be put into”) or longer sentences (e.g., “Aftercooking, consuming food, and preparing for food, where canwe throw away the rest of the food and scraps?”) that involvecomplex reasoning or world knowledge.3.2. BenchmarkGiven the lack of quantitative evaluation, it is imperative toestablish a benchmark for the reasoning segmentation task.To ensure reliable assessment, we have collected a diverseset of images from OpenImages [ 21] and ScanNetv2 [ 10],annotating them with implicit text instructions and high-quality target masks. To cover different scenarios, our textinstructions consist of two types: 1) short phrases; 2) longsentences; as illustrated in Figure 2. The resulting ReasonSegbenchmark comprises a total of 1218 image-instruction-maskdata samples. This dataset is further partitioned into threesplits: train ,val, and test , containing 239, 200, and779 data samples, respectively. As the primary purpose of
the benchmark is evaluation, the validation and testing setsinclude a larger number of data samples. The details of data
779 data samples, respectively. As the primary purpose ofthe benchmark is evaluation, the validation and testing setsinclude a larger number of data samples. The details of dataannotation are given in the supplementary material.4. Our MethodIn this section, we first introduce the model architecturein Sec. 4.1. After that, we elaborate on the training datapreparation and training parameters in Sec. 4.2.4.1. ArchitectureEmbedding as Mask. Most current multimodal LLMs(such as LLaV A [ 29], Flamingo [ 1], BLIP-2 [ 24], Otter [ 23],etc.) support image and text as input, but they can onlyoutput text and cannot directly output fine-grained segmen-tation masks. VisionLLM [ 47] offers a solution by parsing
Multi-ModalLLM LoRA❄ 🔥Vision        Backbone       Decoder🔥It is <SEG>.❄ What is  the food with the mostVitamin C  in this image? Pleaseoutput segmentation mask.🔥Trainable❄ FrozenFigure 3. The pipeline of LISA. Given the input image and text query, the multimodal LLM (e.g., LLaV A [ 29]) generates text output. Thelast-layer embedding for the <SEG> token is then decoded into the segmentation mask via the decoder. We use LoRA [ 15] for efficientfine-tuning. The choice of vision backbone can be flexible (e.g., SAM [66], Mask2Former [9]).segmentation masks as sequences of polygons, enabling therepresentation of segmentation masks as plain text and al-lowing end-to-end training within the framework of existingmultimodal LLMs. However, end-to-end training with thepolygon sequences introduces optimization challenges andmay compromise generalization ability unless a massiveamount of data and computational resources are employed.
For instance, training a 7B model, VisionLLM requires 4×8NVIDIA 80G A100 GPUs and 50 epochs, which is compu-tationally prohibitive. In contrast, it takes less than 3 days totrain LISA-7B on 8 NVIDIA 24G 3090 GPUs.To this end, we propose the embedding-as-mask paradigmto infuse new segmentation capabilities into the multimodalLLM. The pipeline of our method is illustrated in Fig. 3.Specifically, we first expand the original LLM vocabularywith a new token, i.e., <SEG> , which signifies the requestfor the segmentation output. Given a text instruction xtxtalong with the input image ximg, we feed them into themultimodal LLM F, which in turn outputs a text responseˆytxt. It can be formulated asˆytxt=F(ximg,xtxt). (1)When the LLM intends to generate a binary segmentationmask, the output ˆytxtwould include a <SEG> token. Wethen extract the LLM last-layer embedding ˜hsegcorrespond-ing to the <SEG> token and apply an MLP projection layer
γto obtain hseg. Simultaneously, the vision backbone Fencextracts the dense visual features ffrom the visual inputximg. Finally, hsegandfare fed to the decoder Fdectoproduce the final segmentation mask ˆM. The detailed struc-ture of the decoder Fdecfollows [ 19]. The process can beformulated ashseg=γ(˜hseg),f=Fenc(ximg),ˆM=Fdec(hseg,f).(2)Training Objectives. The model is trained end-to-endusing the text generation loss Ltxtand the segmentationmask loss Lmask . The overall objective Lis the weightedsum of these losses, determined by λtxtandλmask :L=λtxtLtxt+λmaskLmask. (3)Specifically, Ltxtis the auto-regressive cross-entropy lossfor text generation, and Lmask is the mask loss, which en-courages the model to produce high-quality segmentationresults. To compute Lmask , we employ a combination ofper-pixel binary cross-entropy (BCE) loss and DICE loss,with corresponding loss weights λbceandλdice. Given theground-truth targets ytxtandM, these losses can be formu-
lated asLtxt=CE(ˆytxt,ytxt),Lmask =λbceBCE (ˆM,M) +λdiceDICE (ˆM,M).(4)It is noteworthy that the proposed method endows exist-ing multimodal LLMs with new segmentation capabilities,such that they can generate not only text but also fine-grainedoutput formats. Also, our method is based on an end-to-endtraining pipeline and connects the LLM and vision moduleswith hidden embedding representation, which proves signifi-cantly more effective than the decoupled two-stage methodas discussed in Sec. 5.2.4.2. TrainingTraining Data Formulation. As illustrated in Fig. 4, ourtraining data comprises mainly three parts, all of which arederived from widely-used public datasets. The details are asfollows:•Semantic Segmentation Dataset. Semantic segmenta-tion datasets typically consist of images and the corre-sponding multi-class labels. During training, we ran-domly choose several categories for each image. Togenerate data that matches the format of visual ques-
tion answering, we employ a question-answer tem-plate like “ USER :<IMAGE> Can you segment the
xx•USER: <IMAGE> Can you segment the tablein this image? •ASSISTANT:It is <SEG>.Raw DataProcessed DataSemantic Segmentation Datathe lady with the blue shirt•USER: <IMAGE> Can you segment the lady with the blue shirtin this image? •ASSISTANT: Sure, it is <SEG>.Referring Segmentation Data•USER: What type of sign and traffic device can be seen in the image? <IMAGE>•ASSISTANT: In the image, there is a street sign and a traffic light above a city road.•USER: How many traffic lights are visible in the image? •…VQA DataNo Binary Segmentation MaskFigure 4. The illustration of training data formulation from different types of data, including semantic segmentation data, referringsegmentation data, and visual question answering (VQA) data.{class name}in this image? ASSISTANT :Itis<SEG> .”, where {class name}is the chosen cat-egory, and <IMAGE> denotes the placeholder for tokensof image patches. The corresponding binary segmenta-tion mask is used as the ground truth to provide mask
loss supervision. During training, we also use other tem-plates to generate the QA data to ensure data diversity, asshown in the supplementary material. We adopt ADE20K,COCO-Stuff, and LVIS-PACO part segmentation datasets.•Vanilla Referring Segmentation Dataset. Referring seg-mentation datasets provide an input image and an ex-plicit short description of the target object. Thus, it iseasy to convert them into question-answer pairs usinga template like “ USER :<IMAGE> Can you segment{description }in this image? ASSISTANT :Sure, it is <SEG> .”, where {description }is thegiven explicit description. For this part, we adopt ref-COCO, refCOCO+, refCOCOg, and refCLEF datasets.•Visual Question Answering Dataset. To preserve the origi-nal Visual Question Answering (VQA) ability of the mul-timodal LLM, we also include the VQA dataset duringtraining. We use LLaV A-Instruct-150k [ 29] for LLaV A v1and LLaV A-v1.5-mix665k for LLaV A v1.5 [28].
Notably, the above datasets do not include any reasoningsegmentation data sample. Instead, it only contains sampleswhere the target objects are explicitly indicated in the querytexts. Surprisingly, even without complex reasoning trainingdata, LISA demonstrates impressive zero-shot ability ontheReasonSeg benchmark, as shown in Table 1. Moreover,we find that further performance boost could be yielded byfinetuning the model on only 239 data samples that involvecomplex reasoning.Trainable Parameters. To preserve the learned knowl-edge of the pre-trained multimodal LLM F(i.e., LLaV A inour experiments), we leverage LoRA [ 15] to perform effi-cient fine-tuning, and completely freeze the vision backboneFenc. The decoder Fdecis fully fine-tuned. Additionally, theLLM token embeddings ( embed tokens ), the LLM head(lmhead ), and the projection layer γare also trainable.It is notable that the resulting model avoids the catas-
trophic forgetting of the original text generation capabilityand preserves the conversation ability, as verified in thesupplementary material. The potential reasons are: we 1)employ LoRA fine-tuning to reduce the trainable parametersand 2) incorporate the VQA dataset during fine-tuning.
Table 1. Reasoning segmentation results among LISA (ours) and previous related works. ‘ft’ denotes using 239 reasoning segmentation datasamples to fine-tune the model. Unless otherwise specified, we use LLaV A v1 [ 29] as the base model. LLaV A1.5 denotes LLaV A v1.5 [ 28].Methodval testoverall short query long query overallgIoU cIoU gIoU cIoU gIoU cIoU gIoU cIoUOVSeg [26] 28.5 18.6 18.0 15.5 28.7 22.5 26.1 20.8GRES [27] 22.4 19.9 17.6 15.0 22.6 23.8 21.3 22.0X-Decoder [65] 22.6 17.9 20.4 11.6 22.2 17.5 21.7 16.3SEEM [66] 25.5 21.2 20.1 11.5 25.6 20.8 24.3 18.7Grounded-SAM [30] 26.0 14.5 17.8 10.8 22.4 18.6 21.3 16.4LISA-7B 44.4 46.0 37.6 34.4 36.6 34.7 36.8 34.1LISA-7B (ft) 52.9 54.0 40.6 40.6 49.4 51.0 47.3 48.4LISA-13B 48.9 46.9 39.9 43.3 46.4 46.5 44.8 45.8LISA-13B (ft) 56.2 62.9 44.3 42.0 54.0 54.3 51.7 51.1LLaV A1.5-7B + OVSeg 38.2 23.5 24.2 18.7 44.6 37.1 39.7 31.8LISA-7B-LLaV A1.5 53.6 52.3 47.1 48.5 49.2 48.9 48.7 48.8
LISA-7B-LLaV A1.5 (ft) 61.3 62.9 48.3 46.3 57.9 59.7 55.6 56.9LLaV A1.5-13B + OVSeg 37.9 26.4 27.1 19.4 46.1 40.6 41.5 34.1LISA-13B-LLaV A1.5 57.7 60.3 50.8 50.0 54.7 50.9 53.8 50.8LISA-13B-LLaV A1.5 (ft) 65.0 72.9 55.4 50.6 63.2 65.3 61.3 62.25. Experiment5.1. Experimental SettingNetwork Architecture. Unless otherwise specified, weuse LLaV A-7B-v1-1 or LLaV A-13B-v1-1 [ 29] as the basemultimodal LLM F, and adopt the ViT-H SAM [ 19] back-bone as the vision backbone Fenc. The projection layer of γis an MLP with channels of [256, 4096, 4096].Implementation Details. We adopt 8 NVIDIA 24G 3090GPUs for training. The training scripts are based on deep-speed [ 41] engine. We use AdamW [ 33] optimizer withthe learning rate and weight decay set to 0.0003 and 0, re-spectively. We also adopt WarmupDecayLR as the learningrate scheduler, where the warmup iterations are set to 100.The weights of the text generation loss λtxtand the mask
lossλmask are set to 1.0and1.0, respectively, and those ofthe bce loss λbceand the dice loss λdiceare set to 2.0and0.5, respectively. Besides, the batch size per device is set to2, and the gradient accumulation step is set to 10. Duringtraining, we select at most 3 categories for each image insemantic segmentation datasets.Datasets. As mentioned in Sec. 4.2, our training data ismainly composed of three types of datasets: (1) For thesemantic segmentation dataset, we use ADE20K [ 62] andCOCO-Stuff [ 3]. Besides, to enhance the segmentation re-sult for some part of an object, we also use part seman-tic segmentation datasets, including PACO-LVIS [ 40], Par-tImageNet [ 13], and PASCAL-Part [ 6]; (2) For the refer-ring segmentation dataset, we use refCLEF, refCOCO, ref-COCO+ [ 17], and refCOCOg [ 35]; (3) For the visual ques-tion answering (VQA) dataset, we use the datasets of LLaV A-Instruct-150k for LLaV A v1 [ 29] and LLaV A-v1.5-mix665k
for LLaV A v1.5 [ 28]. In order to avoid data leakage, weexclude the COCO samples whose images are present in therefCOCO(+/g) validation sets during training. Furthermore,we surprisingly find that by fine-tuning the model on only239 ReasonSeg data samples, the model’s performance canbe further boosted.Evaluation Metrics. We follow most previous works onreferring segmentation [ 17,35] to adopt two metrics: gIoUand cIoU. gIoU is defined by the average of all per-imageIntersection-over-Unions (IoUs), while cIoU is defined bythe cumulative intersection over the cumulative union. SincecIoU is highly biased toward large-area objects and it fluctu-ates too much, gIoU is preferred.5.2. Reasoning Segmentation ResultsThe reasoning segmentation results are shown in Table 1. Itis worth noting that existing works fail to handle the task,but our model can accomplish the task involving complexreasoning with more than 20% gIoU performance boost.
As mentioned before, the reasoning segmentation task isessentially different from the referring segmentation task
reasoning with more than 20% gIoU performance boost.As mentioned before, the reasoning segmentation task isessentially different from the referring segmentation taskin that it requires the model to possess reasoning ability oraccess world knowledge . Only by truly understanding thequery, can the model do well in the task. The existing workshave no proper way to understand an implicit query, but ourmodel exploits multimodal LLMs to reach the goal.Notably, we also make a comparison with the vanillatwo-stage method (LLaV A1.5 + OVSeg). Specifically, the
Table 2. Referring segmentation results (cIoU) among LISA (ours) and existing methods.MethodrefCOCO refCOCO+ refCOCOgval testA testB val testA testB val(U) test(U)MCN [34] 62.4 64.2 59.7 50.6 55.0 44.7 49.2 49.4VLT [11] 67.5 70.5 65.2 56.3 61.0 50.1 55.0 57.7CRIS [48] 70.5 73.2 66.1 62.3 68.1 53.7 59.9 60.4LA VT [53] 72.7 75.8 68.8 62.1 68.4 55.1 61.2 62.1ReLA [27] 73.8 76.5 70.2 66.0 71.0 57.7 65.0 66.0X-Decoder [65] - - - - - - 64.6 -SEEM [66] - - - - - - 65.7 -LISA-7B 74.1 76.5 71.1 62.4 67.4 56.5 66.4 68.5LISA-7B (fine-tuned on ReferSeg) 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6two-stage method refers to first using a multimodal LLM(e.g., LLaV A v1.5) to generate a text output for the inputquery, and then adopting a referring or open-vocabulary seg-mentation model (e.g., OVSeg) to generate the segmentationmask. If the intermediate text output remains too long andexceeds the input token length limit of OVSeg, we use GPT-
3.5 to further summarize. More details can be found in thesupplementary material. The results in Table 1 show thatour model outperforms the two-stage method significantly.We explain that the potential reasons are: 1) Our modelis trained end-to-end, while the two-stage method is com-pletely decoupled; 2) The two-stage method relies on textas an intermediary to transmit information, while our modelutilizes the hidden embedding that is more expressive.Another finding is that LISA-13B outperforms the 7Bcounterpart substantially, especially on the long-query sce-narios, which indicates that the current performance bot-tleneck may still lie in understanding the query text, and astronger multimodal LLM (e.g., LLaV A v1.5 [ 28]) leads toeven better results.5.3. Vanilla Referring Segmentation ResultsTo show that our model is also competent in the vanillareferring segmentation task, we make a comparison withexisting state-of-the-art methods in Table 2. We evaluate
the methods on refCOCO, refCOCO+, refCOCOg validationand testing sets. Our model achieves state-of-the-art resultsacross various referring segmentation benchmarks.5.4. Ablation StudyIn this section, we conduct an extensive ablation study toreveal the contribution of each component. Unless otherwisespecified, we report the metrics of gIoU and cIoU of LISA-7B on the validation set.Design Choices of Vision Backbone. We emphasize thatvision backbones other than SAM are also applicable inour framework. In Table 3, we notice that SAM performsthe best, potentially because of the massive high-qualityTable 3. Ablation study on the design choice of vision backbone.‘ft’ denotes finetuning on ReasonSeg training set.Vision Backbone gIoU cIoUMask2Former-Swin-L 42.4 38.8SAM (w/ LoRA) 41.5 37.3SAM 44.4 46.0Mask2Former-Swin-L (ft) 50.7 52.3SAM w/ LORA (ft) 51.8 51.9SAM (ft) 52.9 54.0Table 4. Ablation study on SAM pre-trained weight and rephrasing.
Exp. ID Pre-train SAMγ rephrasing gIoU cIoU1 ! 35.9 44.62 ! 50.7 51.13 ! ! 52.9 54.0data used in its pre-training phase. Further, we also findthat with the Mask2Former backbone, our framework stillachieves a decent performance on the reasoning segmenta-tion task, significantly outperforming previous works such asX-Decoder [ 65]. This reveals the fact that the design choiceof vision backbone is flexible and not limited to SAM.SAM LoRA Fintuning. We also investigate the effective-ness of applying LoRA on the SAM backbone. In Table 3,we note that the performance of LoRA fine-tuned SAM back-bone is inferior to that of the frozen one. A potential reasonis that fine-tuning impairs the generalization ability of theoriginal SAM model.SAM Pre-trained Weight. To demonstrate the contribu-tion of SAM pre-trained weight, we make a comparisonbetween Experiments 1 and 3 in Table 4. Without being ini-tialized with SAM pre-trained weight, the vision backbone
is trained from scratch. This causes the performance to fallsubstantially behind that of the baseline model.
When a plane is ready to land on the airport runway, what area in the picture will it eventually land on?ImageQueryGTOVSegGRESX-DecoderSEEMOursIn cold days, dogs may need extra protection to keep them warm. What object in the picture can a dog wear to provide warmth during snowy walks?the object used for stirring milk or coffeethe most likely object that someone else has left behindFigure 5. Visual comparison among LISA (ours) and existing related methods. More illustrations are given in the supplementary material.Table 5. Ablation study on training data.IDSemanticSegReferSeg VQA ReasonSeg gIoU cIoUADE20K COCO-Stuff PartSeg1 ! ! !!! 48.9 53.52! ! !!! 48.5 50.83! ! !!! 46.7 50.94 !!!! 46.6 46.75 !!! 30.4 20.46! ! ! !! 47.7 51.17! ! ! !! 44.4 46.08! ! ! !!! 52.9 54.0Table 6. Results on the ReasonSeg test set.Training splits # data samples gIoU cIoUtrain 239 51.7 51.1train + val 439 54.0 54.9Instruction Rephrasing by GPT-3.5. When fine-tuning
the model on the reasoning segmentation data samples, werephrase the text instruction by GPT-3.5 (the details areshown in the supplementary material), and randomly chooseone. The comparison between Experiments 2 and 3 in Ta-ble 4 shows that the performance is increased by 2.2% gIoUand 2.9% cIoU. This result verifies the effectiveness of suchdata augmentation.Contribution of All Types of Training Data. In Table 5,we show the contribution of each type of data to the perfor-mance. We find that in Exp. 5, we do not use any seman-tic segmentation dataset, and the performance drops a lot.We conjecture that semantic segmentation datasets providea large amount of ground-truth binary masks for training,since a multi-class label can induce multiple binary masks.We also notice that adding more reasoning segmentationdata samples during training leads to better results. In Ta-ble 6, we also add the ReasonSeg val set (200 data samples)
during fine-tuning, and it yields better performance in bothgIoU and cIoU metrics. This indicates that more reasoningsegmentation training samples are beneficial at this moment.5.5. Qualitative ResultsAs depicted in Fig. 5, we provide a visual comparisonwith existing related works, including the model for open-vocabulary semantic segmentation (OVSeg), referring seg-mentation (GRES), and the generalist models for segmenta-tion (X-Decoder and SEEM). These models fail to handle thedisplayed cases with various errors, while our approach pro-duces accurate and high-quality segmentation results. Moreillustrations are given in the supplementary material.6. ConclusionIn this work, we have proposed a new segmentationtask— reasoning segmentation . Also, we have introducedan evaluation benchmark ReasonSeg , which comprises overone thousand data samples. Finally, we have presented ourmodel — LISA. It injects segmentation capabilities into cur-
rent multimodal LLMs and performs surprisingly effectivelyon the reasoning segmentation task. We hope our work canshed new light on the direction of combining LLMs andvision tasks in the future.AcknowledgementsThis work is supported in part by the Research Grants Coun-cil under the Areas of Excellence scheme grant AoE/E-601/22-R and the Shenzhen Science and Technology Pro-gram under No. KQTD20210811090149095.
References[1]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, AntoineMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,Katherine Millican, Malcolm Reynolds, et al. Flamingo: avisual language model for few-shot learning. NeurIPS , 2022.1, 3[2]Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.Segnet: A deep convolutional encoder-decoder architecturefor image segmentation. TPAMI, 2017. 2[3]Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In CVPR , 2018.6[4]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, NicolasUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV , 2020.2[5]Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic imagesegmentation with deep convolutional nets, atrous convolu-tion, and fully connected crfs. TPAMI, 2018. 2[6]Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler,
Raquel Urtasun, and Alan Yuille. Detect what you can: De-tecting and representing objects using holistic models andbody parts. In CVPR, 2014. 6[7]Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.Panoptic-deeplab: A simple, strong, and fast baseline forbottom-up panoptic segmentation. In CVPR, 2020. 2[8]Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmenta-tion. NeurIPS, 2021. 2[9]Bowen Cheng, Ishan Misra, Alexander G Schwing, AlexanderKirillov, and Rohit Girdhar. Masked-attention mask trans-former for universal image segmentation. In CVPR , 2022. 2,4[10] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber,Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR ,2017. 2, 3[11] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang.Vision-language transformer and query generation for refer-
ring segmentation. In ICCV, 2021. 7[12] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, ZhiweiFang, and Hanqing Lu. Dual attention network for scenesegmentation. In CVPR, 2019. 2[13] Ju He, Shuo Yang, Shaokang Yang, Adam Kortylewski, Xi-aoding Yuan, Jie-Neng Chen, Shuai Liu, Cheng Yang, QihangYu, and Alan Yuille. Partimagenet: A large, high-qualitydataset of parts. In ECCV, 2022. 6[14] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-shick. Mask r-cnn. In ICCV, 2017. 2[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and WeizhuChen. Lora: Low-rank adaptation of large language mod-els.arXiv:2106.09685, 2021. 4, 5[16] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang,Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attentionfor semantic segmentation. In ICCV, 2019. 2[17] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, andTamara Berg. Referitgame: Referring to objects in pho-
tographs of natural scenes. In EMNLP, 2014. 2, 3, 6[18] Alexander Kirillov, Kaiming He, Ross Girshick, CarstenRother, and Piotr Doll ´ar. Panoptic segmentation. In CVPR ,2019. 2[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-thing. arXiv:2304.02643, 2023. 2, 4, 6[20] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.Grounding language models to images for multimodal inputsand outputs. 2023. 3[21] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings,Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov,Matteo Malloci, Alexander Kolesnikov, Tom Duerig, andVittorio Ferrari. The open images dataset v4: Unified im-age classification, object detection, and visual relationshipdetection at scale. IJCV, 2020. 2, 3[22] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao,Liwei Wang, and Jiaya Jia. Semi-supervised semantic seg-
detection at scale. IJCV, 2020. 2, 3[22] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao,Liwei Wang, and Jiaya Jia. Semi-supervised semantic seg-mentation with directional context-aware consistency. InCVPR, 2021. 2[23] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,Jingkang Yang, and Ziwei Liu. Otter: A multi-modal modelwith in-context instruction tuning. arXiv:2305.03726 , 2023.1, 3[24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen im-age encoders and large language models. arXiv:2301.12597,2023. 1, 3[25] Yanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang,Zeming Li, Jian Sun, and Jiaya Jia. Fully convolutionalnetworks for panoptic segmentation. In CVPR, 2021. 2[26] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, YinanZhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and DianaMarculescu. Open-vocabulary semantic segmentation withmask-adapted clip. In CVPR, 2023. 6
mask-adapted clip. In CVPR, 2023. 6[27] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: General-ized referring expression segmentation. In CVPR , 2023. 6,7[28] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.Improved baselines with visual instruction tuning. arXivpreprint, 2023. 1, 5, 6, 7[29] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.Visual instruction tuning. arXiv:2304.08485 , 2023. 1, 3, 4, 5,6[30] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, HaoZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, JunZhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint , 2023.6[31] Wei Liu, Andrew Rabinovich, and Alexander C. Berg.Parsenet: Looking wider to see better. arXiv preprint , 2015.2
[32] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang,Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang,Qingyun Li, Jiashuo Yu, et al. Internchat: Solving vision-centric tasks by interacting with chatbots beyond language.arXiv:2305.05662, 2023. 3[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decayregularization. arXiv:1711.05101, 2017. 6[34] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, ChenglinWu, Cheng Deng, and Rongrong Ji. Multi-task collaborativenetwork for joint referring expression comprehension andsegmentation. In CVPR, 2020. 7[35] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Cam-buru, Alan L Yuille, and Kevin Murphy. Generation and com-prehension of unambiguous object descriptions. In CVPR ,2016. 6[36] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Mod-eling context between objects for referring expression under-standing. In ECCV, 2016. 2[37] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learn-
ing deconvolution network for semantic segmentation. InICCV, 2015. 2[38] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shao-han Huang, Shuming Ma, and Furu Wei. Kosmos-2:Grounding multimodal large language models to the world.arXiv:2306.14824, 2023. 3[39] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong,Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, LingpengKong, and Tong Zhang. Detgpt: Detect what you need viareasoning. arXiv:2305.14167, 2023. 3[40] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen,Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez,Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts andattributes of common objects. In CVPR, 2023. 6[41] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yux-iong He. Deepspeed: System optimizations enable trainingdeep learning models with over 100 billion parameters. InSIGKDD, 2020. 6[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.InMICCAI, 2015. 2[43] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fullyconvolutional networks for semantic segmentation. TPAMI ,2017. 2[44] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,Weiming Lu, and Yueting Zhuang. Hugginggpt: Solv-ing ai tasks with chatgpt and its friends in huggingface.arXiv:2303.17580, 2023. 3[45] Zhuotao Tian, Pengguang Chen, Xin Lai, Li Jiang, Shu Liu,Hengshuang Zhao, Bei Yu, Ming-Chang Yang, and Jiaya Jia.Adaptive perspective distillation for semantic segmentation.TPAMI, 2022. 2[46] Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai,Yixin Chen, Shu Liu, and Jiaya Jia. Learning context-awareclassifier for semantic segmentation. AAAI, 2023. 2[47] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, YuQiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. arXiv:2305.11175 ,
2023. 3[48] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, YandongGuo, Mingming Gong, and Tongliang Liu. Cris: Clip-drivenreferring image segmentation. In CVPR, 2022. 7[49] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,Zecheng Tang, and Nan Duan. Visual chatgpt: Talk-ing, drawing and editing with visual foundation models.arXiv:2303.04671, 2023. 3[50] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, MinBai, Ersin Yumer, and Raquel Urtasun. Upsnet: A unifiedpanoptic segmentation network. In CVPR, 2019. 2[51] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and KuiyuanYang. Denseaspp for semantic segmentation in street scenes.InCVPR, 2018. 2[52] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, XiuLi, and Ying Shan. Gpt4tools: Teaching large language modelto use tools via self-instruction. arXiv:2305.18752, 2023. 3[53] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-shuang Zhao, and Philip HS Torr. Lavt: Language-aware vi-
sion transformer for referring image segmentation. In CVPR ,2022. 7[54] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
shuang Zhao, and Philip HS Torr. Lavt: Language-aware vi-sion transformer for referring image segmentation. In CVPR ,2022. 7[54] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, EhsanAzarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, MichaelZeng, and Lijuan Wang. Mm-react: Prompting chatgpt formultimodal reasoning and action. arXiv:2303.11381 , 2023.3[55] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,Yaya Shi, et al. mplug-owl: Modularization empowers largelanguage models with multimodality. arXiv:2304.14178 ,2023. 1, 3[56] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-tion by dilated convolutions. In ICLR, 2016. 2[57] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, WenqiShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi:Instruction tuning large language model on region-of-interest.arXiv:2307.03601, 2023. 3[58] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change
Loy. K-net: Towards unified image segmentation. NeurIPS ,2021. 2[59] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, XiaogangWang, and Jiaya Jia. Pyramid scene parsing network. InCVPR, 2017. 2[60] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, JianpingShi, and Jiaya Jia. Icnet for real-time semantic segmentationon high-resolution images. In ECCV, 2018.[61] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi,Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise spatial attention network for scene parsing. In ECCV ,2018. 2[62] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-riuso, and Antonio Torralba. Scene parsing through ade20kdataset. In CVPR, 2017. 6[63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, andMohamed Elhoseiny. Minigpt-4: Enhancing vision-
language understanding with advanced large language models.arXiv:2304.10592, 2023. 1, 3[64] Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xi-ang Bai. Asymmetric non-local neural networks for semanticsegmentation. In ICCV, 2019. 2[65] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li,Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang,Lu Yuan, et al. Generalized decoding for pixel, image, andlanguage. In CVPR, 2023. 2, 6, 7[66] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li,Jianfeng Gao, and Yong Jae Lee. Segment everything every-where all at once. arXiv:2304.06718, 2023. 2, 4, 6, 7
Continuous Object State Recognition for Cooking Robots UsingPre-Trained Vision-Language Models and Black-box OptimizationKento Kawaharazuka1, Naoaki Kanazawa1, Yoshiki Obinata1, Kei Okada1, and Masayuki Inaba1Abstract — The state recognition of the environment andobjects by robots is generally based on the judgement of thecurrent state as a classification problem. On the other hand,state changes of food in cooking happen continuously andneed to be captured not only at a certain time point butalso continuously over time. In addition, the state changes offood are complex and cannot be easily described by manualprogramming. Therefore, we propose a method to recognize thecontinuous state changes of food for cooking robots through thespoken language using pre-trained large-scale vision-languagemodels. By using models that can compute the similaritybetween images and texts continuously over time, we cancapture the state changes of food while cooking. We also show
that by adjusting the weighting of each text prompt based onfitting the similarity changes to a sigmoid function and thenperforming black-box optimization, more accurate and robustcontinuous state recognition can be achieved. We demonstratethe effectiveness and limitations of this method by performingthe recognition of water boiling, butter melting, egg cooking,and onion stir-frying.I. INTRODUCTIONState recognition of the environment and objects by robotsis essential for various tasks such as daily life support,security, and disaster response. This includes recognition ofthe open/closed state of doors, the on/off state of lights, andthe relationships among objects, all of which determine thestate of objects at a certain time [1]–[3]. On the other hand,changes in the state of food, as represented by cooking,happen continuously and need to be captured not onlyat a certain time point but also continuously over time.In addition, the state changes are complex and cannot be
easily described by manual programming. Even if each staterecognition is trained by a neural network, it is difficult tocover the wide variety of state changes that occur in food,and it is necessary to prepare datasets, models, and programsfor each state recognition, which also creates problems inmanaging source codes and computational resources.Various cooking robots have been developed to handlechanges in the state of food. [4] has developed a systemwhere two robots cook pancakes from a recipe. [5] hasdeveloped a system to optimize the quality of omelettecooking by batch bayesian optimization. However, these twosystems do not capture the state changes of food directly, andthe cooking is based on the time of applying heat. Althoughthis has a certain effect, it is important to capture changes inthe state of food directly, taking into account the individual1The authors are with the Department of Mechano-Informatics, Grad-
uate School of Information Science and Technology, The University ofTokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan. [kawaharazuka,kanazawa, obinata, k-okada, inaba]@jsk.t.u-tokyo.ac.jpImage -to-Text Retrieval (ITR)Continuous State RecognitionWater boilingButter melting𝒒1⋯𝒒𝑖⋯𝒒𝑁𝒗1：𝒗𝑡：𝒗𝑇Enc Enc Text𝑄𝑖Image𝑉𝑡𝐸𝑤1⋯𝑤𝑖⋯𝑤𝑁Optimize withBlack -Box OptimizationweightingFig. 1. The concept of this study. We propose a continuous object staterecognition method for cooking robots by using pre-trained large-scalevision-language models and black-box optimization.differences of the food, the differences in heat power, andcooking with unknown recipes. On the other hand, there havebeen some studies that capture changes in the state of foodby using images [6]–[9]. However, these all deal with clas-sification problems based on convolutional neural networks,which determine whether the vegetable is sliced, shredded,chopped, etc., and cannot capture continuous changes in the
state of food. At the same time, since the recognition is based
which determine whether the vegetable is sliced, shredded,chopped, etc., and cannot capture continuous changes in thestate of food. At the same time, since the recognition is basedon a predefined classification, it is difficult to respond tochanges in states that are not included in the classification.It is also difficult to understand the degree of change.Therefore, we propose a method to continuously recognizechanges in the state of food for cooking robots throughthe spoken language using pre-trained large-scale vision-language models (VLMs) [10], [11] as shown in Fig. 1. Inthis study, we utilize VLMs that have learned the semanticcorrespondences between images and the spoken languagethrough a large dataset [12], [13]. By using the spokenlanguage, the proposed method can appropriately capturediverse and ambiguous state changes in the cooking process.Due to the semantic training of correspondences, VLMsare also robust to changes in images. Moreover, by using
pre-trained VLMs, the method does not require any manualprogramming or training of neural networks. Using only asingle VLM makes it easy to manage the source codes andcomputational resources for each state to be recognized. Weprepare a set of various texts about the state of the food tobe recognized, and capture the state changes as continuouschanges in the similarity between the texts and the currentimage. We also show that by adjusting the weighting of eachtext prompt based on fitting the similarity changes to a sig-moid function and then performing black-box optimization,more accurate and robust continuous state recognition can beachieved. The sigmoid function is capable of representingarXiv:2403.08239v1  [cs.RO]  13 Mar 2024
the basic patterns of continuous state changes through itsparameter variations and is well-suited for continuous staterecognition. Our method corresponds to obtaining a valuethat changes more significantly in synchronization with thestate change. It is important to note that no annotationof images is required since only the degree of change iscaptured. We demonstrate the effectiveness and limitationsof our method through experiments on water boiling, buttermelting, egg cooking, and onion stir-frying. Our contribu-tions are summarized as follows:•New Continuous State Recognition Task : Proposinga continuous recognition task for the state changes offood for cooking robots.•Capturing Cooking State Changes : Capturing diverseand ambiguous state changes during the cooking processthrough spoken language analysis.•Simplified Implementation : Eliminating the need formanual programming or neural network training, andensuring easy code management and efficient resource
use with a single pre-trained vision-language model.•Improved Performance : Achieving accurate continu-ous state recognition by adjusting text prompt weightsusing a sigmoid function and black-box optimization.II. R OBOTIC CONTINUOUS STATE RECOGNITION USINGPRE-TRAINED VISION -LANGUAGE MODELS ANDBLACK -BOX OPTIMIZATIONA. Pre-Trained Vision-Language Models for Robotic Contin-uous State RecognitionThere are various types of pre-trained VLMs. Amongthem, it is necessary to obtain the results as continuous valuesrather than discrete ones for continuous state recognition.[10] has classified the tasks that VLMs can handle into fourcategories: Generation Task, Understanding Task, RetrievalTask, and Grounding Task. Generation Task includes Im-age Captioning (IC) and Text-to-Image Generation (TIG).Understanding Task includes Visual Question Answering(VQA), Visual Dialog (VD), Visual Reasoning (VR), andVisual Entailment (VE). Retrieval Task includes Image-to-
Text Retrieval (ITR) and Text-to-Image Retrieval (TIR),which retrieve the correspondence between images and textsfrom alternatives using similarity. Grounding Task includesVisual Grounding (VG), which extracts the correspondingregions in the image from the text. Among these tasks, onlyITR and VG output continuous numerical values, while theothers output sentences or images. Between the two, onlyITR, which can compute the similarity between the currentimage and the texts describing the change in the state offood, is consistent with our purpose.In this study, we conduct experiments using CLIP [12] andImageBind [13] as models that are capable of ITR. CLIP is amodel that can calculate the cosine similarity between imagesand texts by vectorizing them into latent space. ImageBindis a model that can compute similarity not only for imagesand texts, but also for many other modalities including audio,depth images, heatmaps, and inertial sensors. Note that it is
boiled water in the potnot boiling water in the potwater that is boiled in the pot𝒒1：𝒒𝑖：𝒒𝑁𝑡=1Encoder Encoder Encoder𝒗1Encoder𝒗𝑡 𝒗𝑇𝒗1𝑇𝒒1：𝒗1𝑇𝒒𝑖：𝒗1𝑇𝒒𝑁𝒗𝑡𝑇𝒒1：𝒗𝑡𝑇𝒒𝑖：𝒗𝑡𝑇𝒒𝑁𝒗𝑇𝑇𝒒1：𝒗𝑇𝑇𝒒𝑖：𝒗𝑇𝑇𝒒𝑁𝑤1：𝑤𝑖：𝑤𝑁𝒂𝑤1⋯⋯⋯⋯⋯⋯𝒂𝑤𝑡⋯⋯⋯⋯⋯⋯𝒂𝑤𝑇𝑡=𝑇……… …Prepared Text SetData 𝑫𝑄1𝑄𝑖𝑄𝑁𝑉1 𝑉𝑡 𝑉𝑇Text Weight-0.500.511.50 20 40 60 80 100raw average sigmoid-0.500.511.50 20 40 60 80 100raw average sigmoid… …PlotAfter optimizationOptimize withBlack -BoxOptimization𝐸𝐸(𝒘)=𝛼𝛽𝜎𝛼: the slope of the sigmoid𝛽: the center of the sigmoid along 𝑡-axis𝜎: fitting error of the sigmoidFitting of the Sigmoid Function𝑓𝑡=11+𝑒−𝛼(𝑡−𝛽)Calculation of Evaluation ValueCalculation of Similarity ChangesFig. 2. The overview of the proposed method: we obtain time seriesof images D, prepare a variety of text prompts, calculate the continuoussimilarity changes with pre-trained vision-language models and text weight,fit the similarity changes to a sigmoid function, compute the evaluation
similarity changes with pre-trained vision-language models and text weight,fit the similarity changes to a sigmoid function, compute the evaluationvalue, and iteratively optimize the text weight with black-box optimization.necessary to extract the target region for state recognition,which can be done by using VG provided in [14].B. Robotic Continuous State Recognition Using Pre-TrainedVision-Language ModelsContinuous state recognition is performed using VLMscapable of ITR. The method is simple. First, we prepare atext prompt Qfor the state to be recognized. For example,“boiled water” to recognize water boiling and “melted but-ter” to recognize butter melting. Image Vis continuouslyacquired (at 10 Hz in this study), VandQare vectorizedintovandqusing ITR, respectively, and the cosine similarityvTqis calculated. By plotting them continuously over time,we can quantify continuous state changes. In the case ofwater boiling recognition, the similarity of the current image
to the text “boiled water” gradually increases. If we obtainthe moving average of the similarity, we can recognize thebeginning of the state change when the slope of the valuewith respect to time becomes large, or we can recognize theend of the state change when the slope becomes small. Itis also possible to set a certain threshold and recognize thata state change has started or ended when the value exceedsthe threshold.The text input to VLMs does not have to be a single text,but can be multiple synonyms and antonyms (in the caseof antonyms, it is necessary to add a minus sign, −vTq).It is also possible to add noise to the current image andtake the average, or to use multiple models. In [15], wehave experimented with the case where a set of antonymsis prepared and one model is used.C. Robotic Continuous State Recognition Using Black-BoxOptimizationThere are several challenges with the previously de-scribed method. First, preliminary experiments show that
(i)𝑡𝑎𝑤𝑡(iii)𝑡𝑎𝑤𝑡(ii)𝑡𝑎𝑤𝑡(iv)𝑡𝑎𝑤𝑡Fig. 3. Types of continuous state changes. All of these changes can berepresented by a sigmoid function.the recognition performance varies greatly depending onthe choice of texts. By using a variety of texts, we canabsorb the differences in recognition performance amongtexts and obtain stable recognition performance. However, ifdiverse texts are used uniformly, texts with low recognitionperformance may have a negative impact. In addition, therecognition of the beginning and end of state changes relieson human thresholding. It is desirable to have a systemthat automatically obtains high-performance state recognitionwith as little human intervention as possible.Therefore, we propose a method to automatically obtain ahigh-performance state recognizer by adjusting the weightingof various texts, as shown in Fig. 2. We obtain data on thestate changes only once, fit the continuous similarity changes
to a sigmoid function, compute an evaluation function, andadjust the weighting of each text based on black-box opti-mization. By obtaining similarity changes with larger statechanges and smaller variance, accurate and robust continuousstate recognition can be achieved. Here, no annotation ofthe data is required. Moreover, there is no need to preparea model or program for each state recognition, as onlythe text set and weighting need to be changed for eachstate recognition, which facilitates the management of sourcecodes and computational resources.Here, we discuss the reason for using the sigmoid functionfor fitting. As shown in Fig. 3, there are basically fourpossible state changes: (i), (ii), (iii), and (iv) (note that thevertical axis of the graph, atw, is the weighted similarity ofEq. 1 described subsequently). They are (i) the case wherethe state change continues at all times, (ii) the case wherethere is no change at the beginning but a continuous change
happens thereafter, (iii) the case where a state change occursfrom the beginning but the change eventually converges, and(iv) the case where (ii) and (iii) are combined. The sigmoidfunction can represent all of these cases by changing theslope of its shape or by shifting it to the left or right, andis suitable for representing continuous state changes. It isalso useful in that the value range falls within (0,1), whichallows automatic threshold design such that the state changeis considered to have ended when, for example, the valuereaches the 80% change point (0.8).In the following, we describe the optimization procedure.First, the data Dof the state change to be recognized isobtained once. This Dconsists of a time series of images00.10.20.30.40.50.60.70.80.910 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15𝛽=5𝑓(𝑡)𝑡𝛼=0.25𝛼=0.5𝛼=1.0𝛼=2.0𝛼=4.0𝛽=10Fig. 4. Changes in the sigmoid function when changing the parametersαandβ. By increasing α, the sigmoid function becomes steeper, which
means that the state change can be detected more easily. By increasing β,the state change is less likely to be misidentified early in the process.Vt(1≤t≤T, where Tdenotes the number of images).We also prepare a set of texts Qi(1≤i≤N) that describethe state change to be recognized. Here, let Q1i(e.g. “boiledwater” and “melted butter”) be the set of texts that indicatethe change has occurred, and Q−1i(e.g. “unboiled water”and “unmelted butter”) be the set of texts that indicate thechange has not occurred.Next, for each weight wi(1≤i≤N,0≤wi≤1) ofeach text, we set an evaluation function Eto be maximizedbased on black-box optimization. Here, wirepresents theimportance of each text and how to weigh recognition resultscomputed from each text. First, given a weight w, thesimilarity atwbetween the current image Vtand the text setQis calculated as follows,atw:=NXipiwivTtqi/NXiwi (1)where piis a variable that returns 1forQ1iand−1forQ−1
iand−1forQ−1i. It can be said that the similarity vTtqifor each text is
Qis calculated as follows,atw:=NXipiwivTtqi/NXiwi (1)where piis a variable that returns 1forQ1iand−1forQ−1i. It can be said that the similarity vTtqifor each text isweighted by piwi. The continuous change of this value isfitted to a sigmoid function. The sigmoid considered in thisstudy has the following form,f(t) :=11 +e−α(t−β)(2)where αandβare adjustable parameters that determine theshape of the sigmoid. As shown in Fig. 4, the larger the αis, the larger the slope of the sigmoid function becomes, andthe more clearly the similarity changes. βis a parameterthat shifts the center of the sigmoid function along the t-axis, and the larger it is, the less likely the state change ismisidentified early in the process, since the change in f(t)occurs at the end of process. Since 0< f(t)<1for thissigmoid function, the scale of atwmust be changed. In thisstudy, we compute ˆatwwhere atwis scaled as follows,ˆatw:=atw−aminwamaxw−aminw(3)where a{min,max }
w:=atw−aminwamaxw−aminw(3)where a{min,max }w denotes the minimum and maximum val-ues in the moving average of atw(1≤t≤T) over 3 seconds.This makes 0≤ˆatw≤1, which facilitates fitting to f(t).During inference, it is important to note that the entire time
series data Dis not available from the beginning, and so Eq.3 is performed using the computed a{min,max }w during theoptimization process. Assuming that the default parameterof(α, β)is(0.1, T/2), we obtain (α, β)by fitting usingthe nonlinear least-squares method. Note that the constraintsα≥0andβ≥0are imposed in the fitting. From these, wedefine the evaluation function Eto be maximized as follows,E(w) :=αβ/σ (4)where σis the root mean squared error of the fitting. In otherwords, the evaluation function is designed to minimize thefitting error while making the amount of change as large aspossible and ensuring that a large change in f(t)occurs atthe end of the state change as possible.Finally, we perform black-box optimization. In this study,we apply a genetic algorithm using the library DEAP [16]as the algorithm. The gene sequence represented by wiisoptimized based on the maximization of E. The functioncxBlend is used for crossover with a probability of 50%, and
mutGaussian is used for mutation with a probability of 20%with mean 0 and variance 0.1. Individuals are selected by thefunction selTournament, where the tournament size is set to5, the number of individuals is set to 300, and the number ofgenerations is set to 300. The choice of optimization methodis flexible, and we have tried several methods such as Tree-structured Parzen Estimator (TPE) and Covariance MatrixAdaptation Evolution Strategy (CMAES), but we did not ob-serve significant differences in the results. Therefore, we useda common genetic algorithm with minimum computationalcost.III. E XPERIMENTSIn this study, we perform four experiments: recognitionof water boiling, butter melting, egg cooking (fried egg),and onion stir-frying. Each experiment with the preparedtext set Qare shown in Fig. 5. First, we prepare a datasetDoptfor optimization and a dataset Deval for evaluation.The state changes of water, butter, and egg are obtained
at 10 Hz when the mobile robot PR2 looks at the stovethat is heated to a certain degree. For onion stir-frying, theimage of the onion is acquired every time PR2 stirs the panwith a spatula, because the onion would get burned if notstirred constantly. Although all experiments are conductedat the same heat intensity, the length Tof each data isslightly different. For each experiment, the time when thestate change ends is annotated as tdata. Next, we prepare atmost a set of 50 texts Qdescribing the state change for eachexperiment. We prepare a large number of Qby changingthe article, state expression, and expression form. We use fivekinds of articles: “a”, “the”, “this”, “that”, and no article. Forstate expressions, antonyms such as “boiled”/“unboiled” and“melted”/“not melted” are used (synonyms are also used).The expression form is slightly changed, such as “boiledwater” and “water that is boiled” are used.In this study, we conduct experiments using two models,
CLIP and ImageBind. For each model, we evaluate both DoptandDeval datasets in three settings, OPT ,ONE , and ALL .OPT is the result of applying the black-box optimizationproposed in this study. ONE is the result when only the bestQthat maximizes Eis used among the prepared Q. Usingonly one best Qmeans that a state in which only one of theNscalar values in wis 1 and the rest are 0 is created, and thewwith the highest Eis selected. ALL is the result when allthe prepared Qare used equally without optimization. Thismeans that wi= 1 (1≤i≤N). Regarding Dopt, we plotthe transition of ˆatwand its moving average over 3 secondsforOPT ,ONE , and ALL , respectively. Note that the movingaverages are not plotted for the onion stir-frying experiment,since the number of images is small. Regarding Deval, weplotˆatw, which is transformed by each Eq. 3 obtained fromOPT ,ONE , and ALL inDopt, and its moving average. Foreach plot, tdetected is the time when the moving average first
plotˆatw, which is transformed by each Eq. 3 obtained fromOPT ,ONE , and ALL inDopt, and its moving average. Foreach plot, tdetected is the time when the moving average firstexceeds the set threshold Cthre (Cthre= 0.8in this study).Astdiff=|tdetected −tdata|,tdiff should be as small aspossible. For each experiment, the evaluation value Ewhenfitting the change in ˆatwinto the sigmoid function f(t)isdescribed. Note that while Eis appropriate regarding Dopt,it is for reference only regarding Deval, since ˆatwmay notfall between [0, 1] depending on the experiment.Finally, as a cooking experiment utilizing the proposedmethod, the PR2 robot boils water, blanches broccoli, andstir-fries it with melted butter.A. Water Boiling ExperimentThe results of the water boiling experiment are shownin Fig. 6. Here, “raw” is the raw value, “average” is themoving average, “sigmoid” is the result of fitting f(t)toˆatw, “detected” is the function that becomes 1 after tdetected ,
and the red arrow indicates tdata. As for CLIP, the changein similarity of ALL fluctuates and does not change pro-portionally with the boiling state. Thus, the fitting is notsuccessful, the evaluation value Eis zero, and tdiff is large.On the other hand, the change in similarity of ONE graduallyincreases with time, thus Eis larger and tdiff is smallerthan that of ALL . However, because of the gradual changein the similarity, the state is determined to be boiling earlierthan in actuality. In contrast, for OPT , the abrupt changein similarity occurs almost simultaneously with boiling, thusEis the largest and tdiff is quite small (about 1 second).The variance of the similarity changes is also small andstable recognition results are obtained. As for ImageBind,reasonable performance is obtained even for ALL andONE ,andtdiff is relatively small. The change in similarity ofOPT is more stable than that of ALL andONE , indicating
higher performance. Note that the top 5 text prompts andtheir weights are water that is not boiling in the pot (0.12),water that is boiled in pot (0.12), water that is not boiling inthe pot (0.12), water that is not boiling in this pot (0.12), andboiling water in that pot (0.11) (the weight is normalized tobeΣwi= 1).B. Butter Melting ExperimentThe results of the butter melting experiment are shownin Fig. 7. As for CLIP, the change in similarity of ALL
•{, not} {boiled, boiling} water in{, a, the, this, that} pot•water that is {, not} {boiled, boiling} in{, a, the, this, that} pot•{melted, unmelted , not melted} butter in{, a, the, this, that} frying pan•butter that is {melted, unmelted , not melted} in {, a, the, this, that} frying pan•{cooked, uncooked, fried, unfried , raw} egg in {, a, the, this, that} frying pan•egg that is {cooked, uncooked, fried, unfried , raw} in {, a, the, this, that} frying pan•{sauteed , fresh, caramelized, raw, cooked, uncooked, grilled, crisp} onion in {, a, the, this, that} frying panWater Boiling Experiment Butter Melting Experiment Egg Cooking Experiment Onion Stir-frying ExperimentFig. 5. The experimental setup: the text prompts and representative images for water boiling, butter melting, egg cooking, and onion stir-frying experiments.-0.500.511.50 20 40 60 80 100raw average sigmoid detected-0.500.511.50 20 40 60 80 100raw average sigmoid detected-0.500.511.50 20 40 60 80 100
-0.500.511.50 20 40 60 80 100raw average sigmoid detected-0.500.511.50 20 40 60 80raw average sigmoid detected-0.500.511.50 20 40 60 80raw average sigmoid detected-0.500.511.50 20 40 60 80raw average sigmoid detected-0.500.511.50 20 40 60 80 100raw average sigmoid detected-0.500.511.50 20 40 60 80 100raw average sigmoid detected-0.500.511.50 20 40 60 80 100raw average sigmoid detected-0.500.511.50 20 40 60 80raw average sigmoid detected-0.500.511.50 20 40 60 80raw average sigmoid detected-0.500.511.50 20 40 60 80raw average sigmoid detectedImageBind𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙CLIP𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙𝐸=0.0𝐸=44.40𝐸=207.31𝐸=0.0𝐸=24.56𝐸=186.99𝐸=9.10𝐸=5.39𝐸=380.04𝐸=69.82𝐸=0.57𝐸=179.91𝑡𝑑𝑖𝑓𝑓=61.0𝑡𝑑𝑖𝑓𝑓=26.6𝑡𝑑𝑖𝑓𝑓=1.8𝑡𝑑𝑖𝑓𝑓=67.7𝑡𝑑𝑖𝑓𝑓=25.0𝑡𝑑𝑖𝑓𝑓=1.1𝑡𝑑𝑖𝑓𝑓=2.0𝑡𝑑𝑖𝑓𝑓=5.1𝑡𝑑𝑖𝑓𝑓=4.7𝑡𝑑𝑖𝑓𝑓=5.7𝑡𝑑𝑖𝑓𝑓=46.9𝑡𝑑𝑖𝑓𝑓=5.4ALL ONE OPT𝑎𝑤𝑡𝑎𝑤𝑡𝑎𝑤𝑡Time [sec] Time [sec] Time [sec] Time [sec]
Time [sec] Time [sec] Time [sec] Time [sec]Fig. 6. Results of the water boiling experiment. For the two models CLIP and ImageBind, the results of OPT ,ONE , and ALL are shown regardingDoptandDeval. In the graphs, “raw” expresses the raw value of similarity, “average” expresses the moving average of the raw value over 3 seconds,“sigmoid” expresses the sigmoid function fitted to “average”, and “detected” expresses the function that becomes 1 after tdetected . The red arrow showstdata , the annotated time of state change.fluctuates as with Section III-A, and does not change pro-portionally with the degree of melting. Thus, Eis small andtdiff is large. The performance of ONE is better than that ofALL , but the change in similarity still fluctuates. ComparedtoONE ,OPT shows a stable change in the similarity, but thestate change is detected earlier, especially for Dopt. As forImageBind, state changes are recognized with high accuracy
for all settings, thus Eis large and tdiff is small. ALL has alarger variance of similarity changes compared to ONE andOPT , resulting in Ebeing approximately halved. Note thatthe top 5 text prompts and their weights are butter that is notmelted in that frying pan (0.24), butter that is not melted infrying pan (0.24), not melted butter in a frying pan (0.19),not melted butter in that frying pan (0.18), and melted butterin frying pan (0.08).C. Egg Cooking ExperimentThe results of the egg cooking experiment are shownin Fig. 8. As for CLIP, the change in similarity of ALLfluctuates as with Section III-A and Section III-B, and thusEis small and tdiff is large. For Dopt, the performance ofONE andOPT is reasonable and tdiff is small. On the otherhand, the results for Deval are significantly different fromthose for Dopt, and the accuracy is low. As for ImageBind,there is no large difference between DoptandDevalas with
CLIP, but in most cases, the similarity increases significantly
-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detectedTime [sec]ImageBind𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙Time [sec]CLIP𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙ALL ONE OPT-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detected-0.500.511.50 10 20 30 40 50 60raw average sigmoid detectedTime [sec] Time [sec]𝐸=0.00𝐸=7.36𝐸=57.83𝐸=1.02𝐸=4.68𝐸=52.48𝐸=34.91𝐸=65.48𝐸=65.81𝐸=32.06𝐸=53.54𝐸=66.79𝑡𝑑𝑖𝑓𝑓=51.5𝑡𝑑𝑖𝑓𝑓=3.1𝑡𝑑𝑖𝑓𝑓=9.4𝑡𝑑𝑖𝑓𝑓=𝑁/𝐴𝑡𝑑𝑖𝑓𝑓=2.5𝑡𝑑𝑖𝑓𝑓=0.7𝑡𝑑𝑖𝑓𝑓=2.2𝑡𝑑𝑖𝑓𝑓=0.2
𝑡𝑑𝑖𝑓𝑓=2.5𝑡𝑑𝑖𝑓𝑓=0.7𝑡𝑑𝑖𝑓𝑓=2.2𝑡𝑑𝑖𝑓𝑓=0.2𝑡𝑑𝑖𝑓𝑓=0.0𝑡𝑑𝑖𝑓𝑓=0.4𝑡𝑑𝑖𝑓𝑓=0.2𝑡𝑑𝑖𝑓𝑓=0.1𝑎𝑤𝑡𝑎𝑤𝑡𝑎𝑤𝑡Fig. 7. Results of the butter melting experiment. For the two models CLIP and ImageBind, the results of OPT ,ONE , and ALL are shown regardingDoptandDeval. In the graphs, “raw” expresses the raw value of similarity, “average” expresses the moving average of the raw value over 3 seconds,“sigmoid” expresses the sigmoid function fitted to “average”, and “detected” expresses the function that becomes 1 after tdetected . The red arrow showstdata .-0.500.511.522.530 20 40 60 80 100 120 140 160 180raw average sigmoid detected-0.500.511.50 20 40 60 80 100 120 140 160 180raw average sigmoid detected-0.500.511.50 20 40 60 80 100 120 140 160 180raw average sigmoid detected-0.500.511.522.530 20 40 60 80 100 120raw average sigmoid detected-0.500.511.50 20 40 60 80 100 120raw average sigmoid detected-0.500.511.50 20 40 60 80 100 120raw average sigmoid detected-0.500.511.5
raw average sigmoid detected-0.500.511.50 20 40 60 80 100 120 140 160 180raw average sigmoid timing-0.500.511.50 20 40 60 80 100 120 140 160 180raw average sigmoid timing-0.500.511.50 20 40 60 80 100 120 140 160 180raw average sigmoid timing-0.500.511.50 20 40 60 80 100 120raw average sigmoid timing-0.500.511.50 20 40 60 80 100 120raw average sigmoid timing-0.500.511.50 20 40 60 80 100 120raw average sigmoid timingImageBind𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙CLIP𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙𝐸=0.06𝐸=8.17𝐸=36.31𝐸=2.81𝐸=18.75𝐸=51.87𝐸=0.0𝐸=8.75𝐸=28.06𝐸=15.12𝐸=2.67𝐸=76.09𝑡𝑑𝑖𝑓𝑓=109.4𝑡𝑑𝑖𝑓𝑓=18.9𝑡𝑑𝑖𝑓𝑓=0.3𝑡𝑑𝑖𝑓𝑓=95.7𝑡𝑑𝑖𝑓𝑓=𝑁/𝐴𝑡𝑑𝑖𝑓𝑓=92.9𝑡𝑑𝑖𝑓𝑓=120.0𝑡𝑑𝑖𝑓𝑓=98.7𝑡𝑑𝑖𝑓𝑓=101.7𝑡𝑑𝑖𝑓𝑓=82.0𝑡𝑑𝑖𝑓𝑓=40.3𝑡𝑑𝑖𝑓𝑓=84.1ALL ONE OPT𝑎𝑤𝑡𝑎𝑤𝑡𝑎𝑤𝑡Time [sec] Time [sec] Time [sec] Time [sec]Fig. 8. Results of the egg cooking experiment. For the two models CLIP and ImageBind, the results of OPT ,ONE , and ALL are shown regarding Dopt
andDeval. In the graphs, “raw” expresses the raw value of similarity, “average” expresses the moving average of the raw value over 3 seconds, “sigmoid”expresses the sigmoid function fitted to “average”, and “detected” expresses the function that becomes 1 after tdetected . The red arrow shows tdata .in the early phase of the state change and then remainsconstant. Therefore, the state change cannot be detectedproperly, thus tdiff is large. Note that the top 5 text promptsand their weights are raw egg in that frying pan (0.07),
-0.500.511.50 200 400 600 800 1000raw sigmoid detected-0.500.511.50 200 400 600 800 1000raw sigmoid detected-0.500.511.50 200 400 600 800 1000raw sigmoid detected-0.500.511.50 200 400 600 800 1000 1200raw sigmoid detected-0.500.511.50 200 400 600 800 1000 1200raw sigmoid detected-0.500.511.50 200 400 600 800 1000 1200raw sigmoid detectedImageBind𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙CLIP𝐷𝑜𝑝𝑡 𝐷𝑒𝑣𝑎𝑙-0.500.511.50 200 400 600 800 1000raw sigmoid detected-0.500.511.50 200 400 600 800 1000raw sigmoid detected-0.500.511.50 200 400 600 800 1000raw sigmoid detected-0.500.511.50 200 400 600 800 1000 1200raw sigmoid detected-0.500.511.50 200 400 600 800 1000 1200raw sigmoid detected-0.500.511.50 200 400 600 800 1000 1200raw sigmoid detected𝐸=9.16𝐸=7.48𝐸=28.30𝐸=20.39𝐸=4.46𝐸=16.25𝐸=3.29𝐸=12.67𝐸=27.83𝐸=4.93𝐸=0.36𝐸=12.72𝑡𝑑𝑖𝑓𝑓=57.8𝑡𝑑𝑖𝑓𝑓=285.1𝑡𝑑𝑖𝑓𝑓=57.8𝑡𝑑𝑖𝑓𝑓=113.4𝑡𝑑𝑖𝑓𝑓=511.5𝑡𝑑𝑖𝑓𝑓=57.4𝑡𝑑𝑖𝑓𝑓=569.1𝑡𝑑𝑖𝑓𝑓=57.5𝑡𝑑𝑖𝑓𝑓=57.8𝑡𝑑𝑖𝑓𝑓=625.1𝑡𝑑𝑖𝑓𝑓=796.4𝑡𝑑𝑖𝑓𝑓=170.7ALL ONE OPT𝑎𝑤𝑡𝑎𝑤𝑡𝑎𝑤𝑡
𝑡𝑑𝑖𝑓𝑓=796.4𝑡𝑑𝑖𝑓𝑓=170.7ALL ONE OPT𝑎𝑤𝑡𝑎𝑤𝑡𝑎𝑤𝑡Time [sec] Time [sec] Time [sec] Time [sec]Fig. 9. Results of the onion stir-frying experiment. For the two models CLIP and ImageBind, the results of OPT ,ONE , and ALL are shown regardingDoptandDeval. In the graphs, “raw” expresses the raw value of similarity, “sigmoid” expresses the sigmoid function fitted to “raw”, and “detected”expresses the function that becomes 1 after tdetected . The red arrow shows tdata .cooked egg in a frying pan (0.07), cooked egg in the fryingpan(0.07), cooked egg in this frying pan (0.07), and egg thatis fried in frying pan (0.06).D. Onion Stir-frying ExperimentThe results of the onion stir-frying experiment are shownin Fig. 9. Unlike the previous experiments, the number ofimages is small, so there is no “average” and only “raw”is shown. As for CLIP, the changes in similarity of ALLand OPT are stable as the state changes, indicating thatthe recognition is highly accurate. On the other hand, for
ONE , the change in similarity is not as clear as for ALL andOPT , and tdiff is larger. As for ImageBind, the recognitionperformance is not so high for ALL and ONE , since thechange in similarity fluctuates. On the other hand, OPTrecognizes the state change with high accuracy as with CLIP.Note that the top 5 text prompts and their weights are cookedonion in this frying pan (0.36), raw onion in that frying pan(0.19), grilled onion in this frying pan (0.11), cooked onionin frying pan (0.1), and fresh onion in frying pan (0.08).E. Cooking ExperimentThe experimental result is shown in Fig. 10. We place apot with water and a frying pan with butter on the stove. 1⃝The robot PR2 turns on the heat for the pot, and 2⃝whenboiling is detected using the proposed method (ImageBindwith OPT ), 3⃝adds the broccoli in a sieve to the pot.After boiling for 3 minutes, 4⃝the robot takes the broccoliout, turns on the heat for the frying pan, and 5⃝when the
proposed method detects that the butter has melted, 6⃝addsthe broccoli. Finally, 7⃝the robot stir-fries the boiled broccoliin the frying pan and then turns off the heat. A series ofcooking behaviors using the proposed method was realized.IV. D ISCUSSIONThe obtained experimental results are summarized, andtheir properties and limitations are discussed. In the exper-iments, we handled state changes related to water boiling,butter melting, egg cooking, and onion stir-frying, each ofwhich has different properties, and a variety of results wereobtained. The state change of water boiling is similar to (ii)in Fig. 3, and is easy to be recognized because the changeof state occurs at once towards the end of the process. Thestate change of butter melting is close to (iv) and that ofonion stir-frying is close to (i), and both of them can berecognized with high performance for the same reason asabove. On the other hand, the state change of egg cooking is
close to (iii), and the performance is limited because a largestate change occurs at the beginning and the subsequent state
above. On the other hand, the state change of egg cooking isclose to (iii), and the performance is limited because a largestate change occurs at the beginning and the subsequent statechanges are difficult to be recognized. More specifically, thecolor of the egg white first turns from transparent to whiteat once, and subsequently there is only a slight color changein egg yolk. CLIP and ImageBind, which were used in thisstudy, were not able to overcome this problem, but we expectthat the performance will be improved if VLM capable ofdetecting more precise changes is developed in the future.As an overall trend, we found that OPT with black-boxoptimization has higher recognition performance than ALLandONE . Although the performance of ONE is somewhathigher than that of ALL , it may be reversed depending on the
1 2 3 4 5 6 7Fig. 10. Results of the cooking experiment. The PR2 robot boils water, blanches broccoli, and stir-fries it with melted butter.state to be recognized. Finally, when comparing CLIP andImageBind, ImageBind shows relatively more stable changesin similarity. For CLIP, the results of DoptandDeval aresometimes significantly different from each other, while thereare less of these cases for ImageBind. On the other hand,there are cases where CLIP performs better than ImageBind,so it is difficult to say which model is better. We believe thatthe simultaneous use of multiple models in the future willimprove the performance of continuous state recognition bytaking advantage of the characteristics of each model.We discuss the prospects of future research. First, in thisstudy, continuous state recognition is based only on thecorrespondence between images and texts, and there arestill many unused modalities. In particular, video, audio,
and heatmaps are indispensable information for cooking, andhigher performance can be expected by integrating theminto the proposed method. Also, in this study, the text setQis manually created. A more practical system can beconstructed by obtaining this text set automatically. We canuse large-scale language models [17] to obtain multiplesynonyms and antonyms of the state to be recognized. Inaddition, we would like to consider various other methodsin the future, such as changing the viewing area for robotsto focus on, using multiple models described above at thesame time, and taking into account incomplete images [18].V. CONCLUSIONIn this study, we proposed a continuous state recognitionmethod for cooking robots based on the spoken languagethrough pre-trained large-scale vision-language models. Aset of texts related to the state to be recognized is prepared,and the similarity between the current image and texts is
calculated in the temporal direction. In order to make thechanges in similarity easier to use for state recognition, weadjusted the weighting of each text based on black-box op-timization by fitting it to a sigmoid function and calculatingits evaluation value. The sigmoid function is suitable forcontinuous state recognition because it can handle variouspatterns of state changes. The recognition performance withoptimization is much better than that without optimization,and we succeeded in recognizing the states of water boiling,butter melting, and onion stir-frying. On the other hand,the recognition of egg cooking was difficult due to the factthat a large change in the image occurs in the early stageof the recognition, and the subsequent smaller changes aredifficult to be recognized. We used CLIP and ImageBindas large-scale vision-language models, and while ImageBindproduces more stable recognition results over all, each model
has different strengths and weaknesses, and we may considerusing both models in combination in the future.REFERENCES[1] R. T. Chin et al. , “Model-Based Recognition in Robot Vision,” ACMComputing Surveys , vol. 18, no. 1, pp. 67–108, 1986.[2] B. Quintana et al. , “Door detection in 3D coloured point clouds ofindoor environments,” Automation in Construction , vol. 85, pp. 146–166, 2018.[3] K. Kawaharazuka et al. , “VQA-based Robotic State RecognitionOptimized with Genetic Algorithm,” in Proceedings of the 2023IEEE International Conference on Robotics and Automation , 2023,pp. 8306–8311.[4] M. Beetz et al. , “Robotic roommates making pancakes,” in Proceed-ings of the 2011 IEEE-RAS International Conference on HumanoidRobots , 2011, pp. 529–536.[5] K. Junge et al. , “Improving Robotic Cooking Using Batch BayesianOptimization,” IEEE Robotics and Automation Letters , vol. 5, no. 2,pp. 760–765, 2020.[6] R. Paul, “Classifying cooking object’s state using a tuned VGG
convolutional neural network,” arXiv preprint arXiv:1805.09391, 2018.[7] A. B. Jelodar et al. , “Identifying Object States in Cooking-Related
convolutional neural network,” arXiv preprint arXiv:1805.09391, 2018.[7] A. B. Jelodar et al. , “Identifying Object States in Cooking-RelatedImages,” arXiv preprint arXiv:1805.06956, 2018.[8] M. S. Sakib, “Cooking Object’s State Identification Without UsingPretrained Model,” arXiv preprint arXiv:2103.02305, 2021.[9] K. Takata et al. , “Efficient Task/Motion Planning for a Dual-arm Robotfrom Language Instructions and Cooking Images,” in Proceedings ofthe 2022 IEEE/RSJ International Conference on Intelligent Robots andSystems , 2022, pp. 12 058–12 065.[10] F. Li et al. , “Vision-Language Intelligence: Tasks, RepresentationLearning, and Large Models,” arXiv preprint arXiv:2203.01922, 2022.[11] K. Kawaharazuka et al. , “Robotic Applications of Pre-Trained Vision-Language Models to Various Recognition Behaviors (in press),” inProceedings of the 2023 IEEE-RAS International Conference onHumanoid Robots , 2023.
Humanoid Robots , 2023.[12] A. Radford et al. , “Learning Transferable Visual Models From NaturalLanguage Supervision,” arXiv preprint arXiv:2103.00020, 2021.[13] R. Girdhar et al. , “ImageBind: One Embedding Space To Bind ThemAll,” in Proceedings of the 2023 IEEE/CVF International Conferenceon Computer Vision and Pattern Recognition , 2023.[14] P. Wang et al. , “OFA: Unifying Architectures, Tasks, and ModalitiesThrough a Simple Sequence-to-Sequence Learning Framework,” arXivpreprint arXiv:2202.03052, 2022.[15] N. Kanazawa et al. , “Recognition of Heat-Induced Food State Changesby Time-Series Use of Vision-Language Model for Cooking Robot(in press),” in Proceedings of the 18th International Conference onIntellignet Autonomous Systems , 2023.[16] F. Fortin et al. , “DEAP: Evolutionary Algorithms Made Easy,” Journalof Machine Learning Research , vol. 13, pp. 2171–2175, 2012.[17] T. B. Brown et al. , “Language Models are Few-Shot Learners,” arXiv
preprint arXiv:2005.14165, 2020.[18] Y . Yuan et al. , “An Adaptive Divergence-Based Non-Negative LatentFactor Model,” IEEE Transactions on Systems, Man, and Cybernetics:Systems , vol. 53, no. 10, pp. 6475–6487, 2023.
